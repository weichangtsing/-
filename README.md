决策树ID3和C4.5的差别？各自优点？
	决策树的优势在于不需要任何领域知识或参数设置，适合于探测性的知识发现。
	ID3使用信息增益作为特征选择的度量，C4.5采用信息增益比作为特征选择的度量
信息增益：g(D,A)=H(D)−H(D|A) H(D)是数据集D的熵，计算公式
 (统计每一类的频率)
Ck是在数据集D中出现k类的数量，N是样本的数量，K是类别的总数 
H(D|A)是特征A对与数据集D的条件熵，其意义是：在子集Di中Y的分布。
计算方法是 
设特征A是离散的，且有n个不同的取值：{a1,a2,...,an},根据特征A的取值将D划分为n个子集：D1,D2,...,Dn，Ni为对应的Di中的样本数
信息增益比：特征A对训练集D的信息增益比gR(D,A)定义为 
 ratio = g(D,A) / H(D) 
HA(D)刻画了特征A对训练集D的分辨能力
	改进
C4.5继承了ID3的有点，并在以下几个方面对ID3算法进行了改进：

	1 用信息增益比来选择属性，克服了用信息增益选择属性是偏向选择取值多的属性的不足
  从公式出发，信息增益是整个数据集的经验熵与特征A对整个数据集的经验条件熵的差值，信息增益越大即经验条件熵越小，那什么情况下的属性会有极小的的经验条件熵呢？举个极端的例子，如果将身份证号作为一个属性，那么，其实每个人的身份证号都是不相同的，也就是说，有多少个人，就有多少种取值，如果用身份证号这个属性去划分原数据集，那么，原数据集中有多少个样本，就会被划分为多少个子集，这样的话，会导致信息增益公式的第二项整体为0，虽然这种划分毫无意义，但是从信息增益准则来讲，这就是最好的划分属性。其实从概念来讲，就一句话，信息增益表示由于特征A而使得数据集的分类不确定性减少的程度，信息增益大的特征具有更强的分类能力。 
	2 在树的构造过程中进行剪枝
	预剪枝：
Tree-Growth终止的条件以及剪枝策略很多，在CART树中已讲了一些。每个叶子上都是“纯的”不见得就是好事，那样会过拟合（表示每个叶子都是同一种决策结果，纯的）。还有一个方法是叶子节点上覆盖的样本个数小于一个阈值时停止Tree-Growth
	后剪枝：
基于误判的剪枝。这个思路很直接，从bottom到up，尝试把每一子树节点换成叶节点，搞一个测试集来测试替换前后的误判率，如果降低了，那么就这么换了，直至没有任何子树可以替换使得测试数据集的表现得以改进时，算法就可以终止。
悲观剪枝就是递归得估算每个内部节点所覆盖样本节点的误判率。剪枝后该内部节点会变成一个叶子节点该叶子节点的类别为原内部节点的最优叶子节点所决定。然后比较剪枝前后该节点的错误率来决定是否进行剪枝，
如何估计剪枝前分类树内部节点的错误率。
区别：需不需要额外的测试集
	3 能够对连续的属性进行离散化处理
  C4.5是如何处理连续属性的呢？实际上它先把连续属性转换为离散属性再进行处理。虽然本质上属性的取值是连续的，但对于有限的采样数据它是离散的，如果有N条样 本，那么我们有N-1种离散化的方法：<=vj的分到左子树，>vj的分到右子树。计算这N-1种情况下最大的信息增益率。在离散属性上只需要计算1次信息增益率，而在连续属性上却需要计算N-1次，计算量是相当大的。有办法可以减少计算量。对于连续属性先进行排序，只有在决策属性发生改变的地方才需要切开。
	4 能够对不完整的数据进行处理

	C4.5的优点 
- 产生分类的规则易于理解 
- 准确率较高 
C4.5的缺点 
- 在构造树的过程中需要多次对数据集进行扫描和排序，因而导致算法的低效 
- 只适用于能够驻留在内存的数据集，当数据集大的无法在内存容纳是程序无法运行

